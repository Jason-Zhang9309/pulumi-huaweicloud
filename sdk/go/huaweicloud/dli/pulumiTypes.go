// Code generated by the Pulumi Terraform Bridge (tfgen) Tool DO NOT EDIT.
// *** WARNING: Do not edit by hand unless you're certain you know what you are doing! ***

package dli

import (
	"context"
	"reflect"

	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
)

type SparkJobDependentPackage struct {
	// Specifies the user group name.
	// Changing this parameter will submit a new spark job.
	GroupName string `pulumi:"groupName"`
	// Specifies the user group resource for details.
	// Changing this parameter will submit a new spark job.
	// The object structure is documented below.
	Packages []SparkJobDependentPackagePackage `pulumi:"packages"`
}

// SparkJobDependentPackageInput is an input type that accepts SparkJobDependentPackageArgs and SparkJobDependentPackageOutput values.
// You can construct a concrete instance of `SparkJobDependentPackageInput` via:
//
//	SparkJobDependentPackageArgs{...}
type SparkJobDependentPackageInput interface {
	pulumi.Input

	ToSparkJobDependentPackageOutput() SparkJobDependentPackageOutput
	ToSparkJobDependentPackageOutputWithContext(context.Context) SparkJobDependentPackageOutput
}

type SparkJobDependentPackageArgs struct {
	// Specifies the user group name.
	// Changing this parameter will submit a new spark job.
	GroupName pulumi.StringInput `pulumi:"groupName"`
	// Specifies the user group resource for details.
	// Changing this parameter will submit a new spark job.
	// The object structure is documented below.
	Packages SparkJobDependentPackagePackageArrayInput `pulumi:"packages"`
}

func (SparkJobDependentPackageArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkJobDependentPackage)(nil)).Elem()
}

func (i SparkJobDependentPackageArgs) ToSparkJobDependentPackageOutput() SparkJobDependentPackageOutput {
	return i.ToSparkJobDependentPackageOutputWithContext(context.Background())
}

func (i SparkJobDependentPackageArgs) ToSparkJobDependentPackageOutputWithContext(ctx context.Context) SparkJobDependentPackageOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkJobDependentPackageOutput)
}

// SparkJobDependentPackageArrayInput is an input type that accepts SparkJobDependentPackageArray and SparkJobDependentPackageArrayOutput values.
// You can construct a concrete instance of `SparkJobDependentPackageArrayInput` via:
//
//	SparkJobDependentPackageArray{ SparkJobDependentPackageArgs{...} }
type SparkJobDependentPackageArrayInput interface {
	pulumi.Input

	ToSparkJobDependentPackageArrayOutput() SparkJobDependentPackageArrayOutput
	ToSparkJobDependentPackageArrayOutputWithContext(context.Context) SparkJobDependentPackageArrayOutput
}

type SparkJobDependentPackageArray []SparkJobDependentPackageInput

func (SparkJobDependentPackageArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]SparkJobDependentPackage)(nil)).Elem()
}

func (i SparkJobDependentPackageArray) ToSparkJobDependentPackageArrayOutput() SparkJobDependentPackageArrayOutput {
	return i.ToSparkJobDependentPackageArrayOutputWithContext(context.Background())
}

func (i SparkJobDependentPackageArray) ToSparkJobDependentPackageArrayOutputWithContext(ctx context.Context) SparkJobDependentPackageArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkJobDependentPackageArrayOutput)
}

type SparkJobDependentPackageOutput struct{ *pulumi.OutputState }

func (SparkJobDependentPackageOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkJobDependentPackage)(nil)).Elem()
}

func (o SparkJobDependentPackageOutput) ToSparkJobDependentPackageOutput() SparkJobDependentPackageOutput {
	return o
}

func (o SparkJobDependentPackageOutput) ToSparkJobDependentPackageOutputWithContext(ctx context.Context) SparkJobDependentPackageOutput {
	return o
}

// Specifies the user group name.
// Changing this parameter will submit a new spark job.
func (o SparkJobDependentPackageOutput) GroupName() pulumi.StringOutput {
	return o.ApplyT(func(v SparkJobDependentPackage) string { return v.GroupName }).(pulumi.StringOutput)
}

// Specifies the user group resource for details.
// Changing this parameter will submit a new spark job.
// The object structure is documented below.
func (o SparkJobDependentPackageOutput) Packages() SparkJobDependentPackagePackageArrayOutput {
	return o.ApplyT(func(v SparkJobDependentPackage) []SparkJobDependentPackagePackage { return v.Packages }).(SparkJobDependentPackagePackageArrayOutput)
}

type SparkJobDependentPackageArrayOutput struct{ *pulumi.OutputState }

func (SparkJobDependentPackageArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]SparkJobDependentPackage)(nil)).Elem()
}

func (o SparkJobDependentPackageArrayOutput) ToSparkJobDependentPackageArrayOutput() SparkJobDependentPackageArrayOutput {
	return o
}

func (o SparkJobDependentPackageArrayOutput) ToSparkJobDependentPackageArrayOutputWithContext(ctx context.Context) SparkJobDependentPackageArrayOutput {
	return o
}

func (o SparkJobDependentPackageArrayOutput) Index(i pulumi.IntInput) SparkJobDependentPackageOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) SparkJobDependentPackage {
		return vs[0].([]SparkJobDependentPackage)[vs[1].(int)]
	}).(SparkJobDependentPackageOutput)
}

type SparkJobDependentPackagePackage struct {
	// Specifies the resource name of the package.
	// Changing this parameter will submit a new spark job.
	PackageName string `pulumi:"packageName"`
	// Specifies the resource type of the package.
	// Changing this parameter will submit a new spark job.
	Type string `pulumi:"type"`
}

// SparkJobDependentPackagePackageInput is an input type that accepts SparkJobDependentPackagePackageArgs and SparkJobDependentPackagePackageOutput values.
// You can construct a concrete instance of `SparkJobDependentPackagePackageInput` via:
//
//	SparkJobDependentPackagePackageArgs{...}
type SparkJobDependentPackagePackageInput interface {
	pulumi.Input

	ToSparkJobDependentPackagePackageOutput() SparkJobDependentPackagePackageOutput
	ToSparkJobDependentPackagePackageOutputWithContext(context.Context) SparkJobDependentPackagePackageOutput
}

type SparkJobDependentPackagePackageArgs struct {
	// Specifies the resource name of the package.
	// Changing this parameter will submit a new spark job.
	PackageName pulumi.StringInput `pulumi:"packageName"`
	// Specifies the resource type of the package.
	// Changing this parameter will submit a new spark job.
	Type pulumi.StringInput `pulumi:"type"`
}

func (SparkJobDependentPackagePackageArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkJobDependentPackagePackage)(nil)).Elem()
}

func (i SparkJobDependentPackagePackageArgs) ToSparkJobDependentPackagePackageOutput() SparkJobDependentPackagePackageOutput {
	return i.ToSparkJobDependentPackagePackageOutputWithContext(context.Background())
}

func (i SparkJobDependentPackagePackageArgs) ToSparkJobDependentPackagePackageOutputWithContext(ctx context.Context) SparkJobDependentPackagePackageOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkJobDependentPackagePackageOutput)
}

// SparkJobDependentPackagePackageArrayInput is an input type that accepts SparkJobDependentPackagePackageArray and SparkJobDependentPackagePackageArrayOutput values.
// You can construct a concrete instance of `SparkJobDependentPackagePackageArrayInput` via:
//
//	SparkJobDependentPackagePackageArray{ SparkJobDependentPackagePackageArgs{...} }
type SparkJobDependentPackagePackageArrayInput interface {
	pulumi.Input

	ToSparkJobDependentPackagePackageArrayOutput() SparkJobDependentPackagePackageArrayOutput
	ToSparkJobDependentPackagePackageArrayOutputWithContext(context.Context) SparkJobDependentPackagePackageArrayOutput
}

type SparkJobDependentPackagePackageArray []SparkJobDependentPackagePackageInput

func (SparkJobDependentPackagePackageArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]SparkJobDependentPackagePackage)(nil)).Elem()
}

func (i SparkJobDependentPackagePackageArray) ToSparkJobDependentPackagePackageArrayOutput() SparkJobDependentPackagePackageArrayOutput {
	return i.ToSparkJobDependentPackagePackageArrayOutputWithContext(context.Background())
}

func (i SparkJobDependentPackagePackageArray) ToSparkJobDependentPackagePackageArrayOutputWithContext(ctx context.Context) SparkJobDependentPackagePackageArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkJobDependentPackagePackageArrayOutput)
}

type SparkJobDependentPackagePackageOutput struct{ *pulumi.OutputState }

func (SparkJobDependentPackagePackageOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkJobDependentPackagePackage)(nil)).Elem()
}

func (o SparkJobDependentPackagePackageOutput) ToSparkJobDependentPackagePackageOutput() SparkJobDependentPackagePackageOutput {
	return o
}

func (o SparkJobDependentPackagePackageOutput) ToSparkJobDependentPackagePackageOutputWithContext(ctx context.Context) SparkJobDependentPackagePackageOutput {
	return o
}

// Specifies the resource name of the package.
// Changing this parameter will submit a new spark job.
func (o SparkJobDependentPackagePackageOutput) PackageName() pulumi.StringOutput {
	return o.ApplyT(func(v SparkJobDependentPackagePackage) string { return v.PackageName }).(pulumi.StringOutput)
}

// Specifies the resource type of the package.
// Changing this parameter will submit a new spark job.
func (o SparkJobDependentPackagePackageOutput) Type() pulumi.StringOutput {
	return o.ApplyT(func(v SparkJobDependentPackagePackage) string { return v.Type }).(pulumi.StringOutput)
}

type SparkJobDependentPackagePackageArrayOutput struct{ *pulumi.OutputState }

func (SparkJobDependentPackagePackageArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]SparkJobDependentPackagePackage)(nil)).Elem()
}

func (o SparkJobDependentPackagePackageArrayOutput) ToSparkJobDependentPackagePackageArrayOutput() SparkJobDependentPackagePackageArrayOutput {
	return o
}

func (o SparkJobDependentPackagePackageArrayOutput) ToSparkJobDependentPackagePackageArrayOutputWithContext(ctx context.Context) SparkJobDependentPackagePackageArrayOutput {
	return o
}

func (o SparkJobDependentPackagePackageArrayOutput) Index(i pulumi.IntInput) SparkJobDependentPackagePackageOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) SparkJobDependentPackagePackage {
		return vs[0].([]SparkJobDependentPackagePackage)[vs[1].(int)]
	}).(SparkJobDependentPackagePackageOutput)
}

type SqlJobConf struct {
	// Sets the job running timeout interval. If the timeout interval
	// expires, the job is canceled. Unit: `ms`. Changing this parameter will create a new resource.
	DliSqlJobTimeout *int `pulumi:"dliSqlJobTimeout"`
	// Specifies whether DDL and DCL statements are executed
	// asynchronously. The value true indicates that asynchronous execution is enabled. Default value is `false`.
	// Changing this parameter will create a new resource.
	DliSqlSqlasyncEnabled *bool `pulumi:"dliSqlSqlasyncEnabled"`
	// Maximum size of the table that
	// displays all working nodes when a connection is executed. You can set this parameter to -1 to disable the display.
	// Default value is `209715200`. Changing this parameter will create a new resource.
	SparkSqlAutoBroadcastJoinThreshold *int `pulumi:"sparkSqlAutoBroadcastJoinThreshold"`
	// Path of bad records. Changing this parameter will create
	// a new resource.
	SparkSqlBadRecordsPath *string `pulumi:"sparkSqlBadRecordsPath"`
	// In dynamic mode, Spark does not delete
	// the previous partitions and only overwrites the partitions without data during execution. Default value is `false`.
	// Changing this parameter will create a new resource.
	SparkSqlDynamicPartitionOverwriteEnabled *bool `pulumi:"sparkSqlDynamicPartitionOverwriteEnabled"`
	// Maximum number of bytes to be packed into a
	// single partition when a file is read. Default value is `134217728`. Changing this parameter will create a new
	// resource.
	SparkSqlFilesMaxPartitionBytes *int `pulumi:"sparkSqlFilesMaxPartitionBytes"`
	// Maximum number of records to be written
	// into a single file. If the value is zero or negative, there is no limit. Default value is `0`.
	// Changing this parameter will create a new resource.
	SparkSqlMaxRecordsPerFile *int `pulumi:"sparkSqlMaxRecordsPerFile"`
	// Default number of partitions used to filter
	// data for join or aggregation. Default value is `4096`. Changing this parameter will create a new resource.
	SparkSqlShufflePartitions *int `pulumi:"sparkSqlShufflePartitions"`
}

// SqlJobConfInput is an input type that accepts SqlJobConfArgs and SqlJobConfOutput values.
// You can construct a concrete instance of `SqlJobConfInput` via:
//
//	SqlJobConfArgs{...}
type SqlJobConfInput interface {
	pulumi.Input

	ToSqlJobConfOutput() SqlJobConfOutput
	ToSqlJobConfOutputWithContext(context.Context) SqlJobConfOutput
}

type SqlJobConfArgs struct {
	// Sets the job running timeout interval. If the timeout interval
	// expires, the job is canceled. Unit: `ms`. Changing this parameter will create a new resource.
	DliSqlJobTimeout pulumi.IntPtrInput `pulumi:"dliSqlJobTimeout"`
	// Specifies whether DDL and DCL statements are executed
	// asynchronously. The value true indicates that asynchronous execution is enabled. Default value is `false`.
	// Changing this parameter will create a new resource.
	DliSqlSqlasyncEnabled pulumi.BoolPtrInput `pulumi:"dliSqlSqlasyncEnabled"`
	// Maximum size of the table that
	// displays all working nodes when a connection is executed. You can set this parameter to -1 to disable the display.
	// Default value is `209715200`. Changing this parameter will create a new resource.
	SparkSqlAutoBroadcastJoinThreshold pulumi.IntPtrInput `pulumi:"sparkSqlAutoBroadcastJoinThreshold"`
	// Path of bad records. Changing this parameter will create
	// a new resource.
	SparkSqlBadRecordsPath pulumi.StringPtrInput `pulumi:"sparkSqlBadRecordsPath"`
	// In dynamic mode, Spark does not delete
	// the previous partitions and only overwrites the partitions without data during execution. Default value is `false`.
	// Changing this parameter will create a new resource.
	SparkSqlDynamicPartitionOverwriteEnabled pulumi.BoolPtrInput `pulumi:"sparkSqlDynamicPartitionOverwriteEnabled"`
	// Maximum number of bytes to be packed into a
	// single partition when a file is read. Default value is `134217728`. Changing this parameter will create a new
	// resource.
	SparkSqlFilesMaxPartitionBytes pulumi.IntPtrInput `pulumi:"sparkSqlFilesMaxPartitionBytes"`
	// Maximum number of records to be written
	// into a single file. If the value is zero or negative, there is no limit. Default value is `0`.
	// Changing this parameter will create a new resource.
	SparkSqlMaxRecordsPerFile pulumi.IntPtrInput `pulumi:"sparkSqlMaxRecordsPerFile"`
	// Default number of partitions used to filter
	// data for join or aggregation. Default value is `4096`. Changing this parameter will create a new resource.
	SparkSqlShufflePartitions pulumi.IntPtrInput `pulumi:"sparkSqlShufflePartitions"`
}

func (SqlJobConfArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*SqlJobConf)(nil)).Elem()
}

func (i SqlJobConfArgs) ToSqlJobConfOutput() SqlJobConfOutput {
	return i.ToSqlJobConfOutputWithContext(context.Background())
}

func (i SqlJobConfArgs) ToSqlJobConfOutputWithContext(ctx context.Context) SqlJobConfOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SqlJobConfOutput)
}

func (i SqlJobConfArgs) ToSqlJobConfPtrOutput() SqlJobConfPtrOutput {
	return i.ToSqlJobConfPtrOutputWithContext(context.Background())
}

func (i SqlJobConfArgs) ToSqlJobConfPtrOutputWithContext(ctx context.Context) SqlJobConfPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SqlJobConfOutput).ToSqlJobConfPtrOutputWithContext(ctx)
}

// SqlJobConfPtrInput is an input type that accepts SqlJobConfArgs, SqlJobConfPtr and SqlJobConfPtrOutput values.
// You can construct a concrete instance of `SqlJobConfPtrInput` via:
//
//	        SqlJobConfArgs{...}
//
//	or:
//
//	        nil
type SqlJobConfPtrInput interface {
	pulumi.Input

	ToSqlJobConfPtrOutput() SqlJobConfPtrOutput
	ToSqlJobConfPtrOutputWithContext(context.Context) SqlJobConfPtrOutput
}

type sqlJobConfPtrType SqlJobConfArgs

func SqlJobConfPtr(v *SqlJobConfArgs) SqlJobConfPtrInput {
	return (*sqlJobConfPtrType)(v)
}

func (*sqlJobConfPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**SqlJobConf)(nil)).Elem()
}

func (i *sqlJobConfPtrType) ToSqlJobConfPtrOutput() SqlJobConfPtrOutput {
	return i.ToSqlJobConfPtrOutputWithContext(context.Background())
}

func (i *sqlJobConfPtrType) ToSqlJobConfPtrOutputWithContext(ctx context.Context) SqlJobConfPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SqlJobConfPtrOutput)
}

type SqlJobConfOutput struct{ *pulumi.OutputState }

func (SqlJobConfOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SqlJobConf)(nil)).Elem()
}

func (o SqlJobConfOutput) ToSqlJobConfOutput() SqlJobConfOutput {
	return o
}

func (o SqlJobConfOutput) ToSqlJobConfOutputWithContext(ctx context.Context) SqlJobConfOutput {
	return o
}

func (o SqlJobConfOutput) ToSqlJobConfPtrOutput() SqlJobConfPtrOutput {
	return o.ToSqlJobConfPtrOutputWithContext(context.Background())
}

func (o SqlJobConfOutput) ToSqlJobConfPtrOutputWithContext(ctx context.Context) SqlJobConfPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v SqlJobConf) *SqlJobConf {
		return &v
	}).(SqlJobConfPtrOutput)
}

// Sets the job running timeout interval. If the timeout interval
// expires, the job is canceled. Unit: `ms`. Changing this parameter will create a new resource.
func (o SqlJobConfOutput) DliSqlJobTimeout() pulumi.IntPtrOutput {
	return o.ApplyT(func(v SqlJobConf) *int { return v.DliSqlJobTimeout }).(pulumi.IntPtrOutput)
}

// Specifies whether DDL and DCL statements are executed
// asynchronously. The value true indicates that asynchronous execution is enabled. Default value is `false`.
// Changing this parameter will create a new resource.
func (o SqlJobConfOutput) DliSqlSqlasyncEnabled() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v SqlJobConf) *bool { return v.DliSqlSqlasyncEnabled }).(pulumi.BoolPtrOutput)
}

// Maximum size of the table that
// displays all working nodes when a connection is executed. You can set this parameter to -1 to disable the display.
// Default value is `209715200`. Changing this parameter will create a new resource.
func (o SqlJobConfOutput) SparkSqlAutoBroadcastJoinThreshold() pulumi.IntPtrOutput {
	return o.ApplyT(func(v SqlJobConf) *int { return v.SparkSqlAutoBroadcastJoinThreshold }).(pulumi.IntPtrOutput)
}

// Path of bad records. Changing this parameter will create
// a new resource.
func (o SqlJobConfOutput) SparkSqlBadRecordsPath() pulumi.StringPtrOutput {
	return o.ApplyT(func(v SqlJobConf) *string { return v.SparkSqlBadRecordsPath }).(pulumi.StringPtrOutput)
}

// In dynamic mode, Spark does not delete
// the previous partitions and only overwrites the partitions without data during execution. Default value is `false`.
// Changing this parameter will create a new resource.
func (o SqlJobConfOutput) SparkSqlDynamicPartitionOverwriteEnabled() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v SqlJobConf) *bool { return v.SparkSqlDynamicPartitionOverwriteEnabled }).(pulumi.BoolPtrOutput)
}

// Maximum number of bytes to be packed into a
// single partition when a file is read. Default value is `134217728`. Changing this parameter will create a new
// resource.
func (o SqlJobConfOutput) SparkSqlFilesMaxPartitionBytes() pulumi.IntPtrOutput {
	return o.ApplyT(func(v SqlJobConf) *int { return v.SparkSqlFilesMaxPartitionBytes }).(pulumi.IntPtrOutput)
}

// Maximum number of records to be written
// into a single file. If the value is zero or negative, there is no limit. Default value is `0`.
// Changing this parameter will create a new resource.
func (o SqlJobConfOutput) SparkSqlMaxRecordsPerFile() pulumi.IntPtrOutput {
	return o.ApplyT(func(v SqlJobConf) *int { return v.SparkSqlMaxRecordsPerFile }).(pulumi.IntPtrOutput)
}

// Default number of partitions used to filter
// data for join or aggregation. Default value is `4096`. Changing this parameter will create a new resource.
func (o SqlJobConfOutput) SparkSqlShufflePartitions() pulumi.IntPtrOutput {
	return o.ApplyT(func(v SqlJobConf) *int { return v.SparkSqlShufflePartitions }).(pulumi.IntPtrOutput)
}

type SqlJobConfPtrOutput struct{ *pulumi.OutputState }

func (SqlJobConfPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**SqlJobConf)(nil)).Elem()
}

func (o SqlJobConfPtrOutput) ToSqlJobConfPtrOutput() SqlJobConfPtrOutput {
	return o
}

func (o SqlJobConfPtrOutput) ToSqlJobConfPtrOutputWithContext(ctx context.Context) SqlJobConfPtrOutput {
	return o
}

func (o SqlJobConfPtrOutput) Elem() SqlJobConfOutput {
	return o.ApplyT(func(v *SqlJobConf) SqlJobConf {
		if v != nil {
			return *v
		}
		var ret SqlJobConf
		return ret
	}).(SqlJobConfOutput)
}

// Sets the job running timeout interval. If the timeout interval
// expires, the job is canceled. Unit: `ms`. Changing this parameter will create a new resource.
func (o SqlJobConfPtrOutput) DliSqlJobTimeout() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *SqlJobConf) *int {
		if v == nil {
			return nil
		}
		return v.DliSqlJobTimeout
	}).(pulumi.IntPtrOutput)
}

// Specifies whether DDL and DCL statements are executed
// asynchronously. The value true indicates that asynchronous execution is enabled. Default value is `false`.
// Changing this parameter will create a new resource.
func (o SqlJobConfPtrOutput) DliSqlSqlasyncEnabled() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *SqlJobConf) *bool {
		if v == nil {
			return nil
		}
		return v.DliSqlSqlasyncEnabled
	}).(pulumi.BoolPtrOutput)
}

// Maximum size of the table that
// displays all working nodes when a connection is executed. You can set this parameter to -1 to disable the display.
// Default value is `209715200`. Changing this parameter will create a new resource.
func (o SqlJobConfPtrOutput) SparkSqlAutoBroadcastJoinThreshold() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *SqlJobConf) *int {
		if v == nil {
			return nil
		}
		return v.SparkSqlAutoBroadcastJoinThreshold
	}).(pulumi.IntPtrOutput)
}

// Path of bad records. Changing this parameter will create
// a new resource.
func (o SqlJobConfPtrOutput) SparkSqlBadRecordsPath() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *SqlJobConf) *string {
		if v == nil {
			return nil
		}
		return v.SparkSqlBadRecordsPath
	}).(pulumi.StringPtrOutput)
}

// In dynamic mode, Spark does not delete
// the previous partitions and only overwrites the partitions without data during execution. Default value is `false`.
// Changing this parameter will create a new resource.
func (o SqlJobConfPtrOutput) SparkSqlDynamicPartitionOverwriteEnabled() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *SqlJobConf) *bool {
		if v == nil {
			return nil
		}
		return v.SparkSqlDynamicPartitionOverwriteEnabled
	}).(pulumi.BoolPtrOutput)
}

// Maximum number of bytes to be packed into a
// single partition when a file is read. Default value is `134217728`. Changing this parameter will create a new
// resource.
func (o SqlJobConfPtrOutput) SparkSqlFilesMaxPartitionBytes() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *SqlJobConf) *int {
		if v == nil {
			return nil
		}
		return v.SparkSqlFilesMaxPartitionBytes
	}).(pulumi.IntPtrOutput)
}

// Maximum number of records to be written
// into a single file. If the value is zero or negative, there is no limit. Default value is `0`.
// Changing this parameter will create a new resource.
func (o SqlJobConfPtrOutput) SparkSqlMaxRecordsPerFile() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *SqlJobConf) *int {
		if v == nil {
			return nil
		}
		return v.SparkSqlMaxRecordsPerFile
	}).(pulumi.IntPtrOutput)
}

// Default number of partitions used to filter
// data for join or aggregation. Default value is `4096`. Changing this parameter will create a new resource.
func (o SqlJobConfPtrOutput) SparkSqlShufflePartitions() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *SqlJobConf) *int {
		if v == nil {
			return nil
		}
		return v.SparkSqlShufflePartitions
	}).(pulumi.IntPtrOutput)
}

type TableColumn struct {
	// Specifies the description of column. Changing this parameter will
	// create a new resource.
	Description *string `pulumi:"description"`
	// Specifies whether the column is a partition column. The value
	// `true` indicates a partition column, and the value false indicates a non-partition column. The default value
	// is false. Changing this parameter will create a new resource.
	IsPartition *bool `pulumi:"isPartition"`
	// Specifies the name of column. Changing this parameter will create a new
	// resource.
	Name string `pulumi:"name"`
	// Specifies data type of column. Changing this parameter will create a new
	// resource.
	Type string `pulumi:"type"`
}

// TableColumnInput is an input type that accepts TableColumnArgs and TableColumnOutput values.
// You can construct a concrete instance of `TableColumnInput` via:
//
//	TableColumnArgs{...}
type TableColumnInput interface {
	pulumi.Input

	ToTableColumnOutput() TableColumnOutput
	ToTableColumnOutputWithContext(context.Context) TableColumnOutput
}

type TableColumnArgs struct {
	// Specifies the description of column. Changing this parameter will
	// create a new resource.
	Description pulumi.StringPtrInput `pulumi:"description"`
	// Specifies whether the column is a partition column. The value
	// `true` indicates a partition column, and the value false indicates a non-partition column. The default value
	// is false. Changing this parameter will create a new resource.
	IsPartition pulumi.BoolPtrInput `pulumi:"isPartition"`
	// Specifies the name of column. Changing this parameter will create a new
	// resource.
	Name pulumi.StringInput `pulumi:"name"`
	// Specifies data type of column. Changing this parameter will create a new
	// resource.
	Type pulumi.StringInput `pulumi:"type"`
}

func (TableColumnArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*TableColumn)(nil)).Elem()
}

func (i TableColumnArgs) ToTableColumnOutput() TableColumnOutput {
	return i.ToTableColumnOutputWithContext(context.Background())
}

func (i TableColumnArgs) ToTableColumnOutputWithContext(ctx context.Context) TableColumnOutput {
	return pulumi.ToOutputWithContext(ctx, i).(TableColumnOutput)
}

// TableColumnArrayInput is an input type that accepts TableColumnArray and TableColumnArrayOutput values.
// You can construct a concrete instance of `TableColumnArrayInput` via:
//
//	TableColumnArray{ TableColumnArgs{...} }
type TableColumnArrayInput interface {
	pulumi.Input

	ToTableColumnArrayOutput() TableColumnArrayOutput
	ToTableColumnArrayOutputWithContext(context.Context) TableColumnArrayOutput
}

type TableColumnArray []TableColumnInput

func (TableColumnArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]TableColumn)(nil)).Elem()
}

func (i TableColumnArray) ToTableColumnArrayOutput() TableColumnArrayOutput {
	return i.ToTableColumnArrayOutputWithContext(context.Background())
}

func (i TableColumnArray) ToTableColumnArrayOutputWithContext(ctx context.Context) TableColumnArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(TableColumnArrayOutput)
}

type TableColumnOutput struct{ *pulumi.OutputState }

func (TableColumnOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*TableColumn)(nil)).Elem()
}

func (o TableColumnOutput) ToTableColumnOutput() TableColumnOutput {
	return o
}

func (o TableColumnOutput) ToTableColumnOutputWithContext(ctx context.Context) TableColumnOutput {
	return o
}

// Specifies the description of column. Changing this parameter will
// create a new resource.
func (o TableColumnOutput) Description() pulumi.StringPtrOutput {
	return o.ApplyT(func(v TableColumn) *string { return v.Description }).(pulumi.StringPtrOutput)
}

// Specifies whether the column is a partition column. The value
// `true` indicates a partition column, and the value false indicates a non-partition column. The default value
// is false. Changing this parameter will create a new resource.
func (o TableColumnOutput) IsPartition() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v TableColumn) *bool { return v.IsPartition }).(pulumi.BoolPtrOutput)
}

// Specifies the name of column. Changing this parameter will create a new
// resource.
func (o TableColumnOutput) Name() pulumi.StringOutput {
	return o.ApplyT(func(v TableColumn) string { return v.Name }).(pulumi.StringOutput)
}

// Specifies data type of column. Changing this parameter will create a new
// resource.
func (o TableColumnOutput) Type() pulumi.StringOutput {
	return o.ApplyT(func(v TableColumn) string { return v.Type }).(pulumi.StringOutput)
}

type TableColumnArrayOutput struct{ *pulumi.OutputState }

func (TableColumnArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]TableColumn)(nil)).Elem()
}

func (o TableColumnArrayOutput) ToTableColumnArrayOutput() TableColumnArrayOutput {
	return o
}

func (o TableColumnArrayOutput) ToTableColumnArrayOutputWithContext(ctx context.Context) TableColumnArrayOutput {
	return o
}

func (o TableColumnArrayOutput) Index(i pulumi.IntInput) TableColumnOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) TableColumn {
		return vs[0].([]TableColumn)[vs[1].(int)]
	}).(TableColumnOutput)
}

func init() {
	pulumi.RegisterInputType(reflect.TypeOf((*SparkJobDependentPackageInput)(nil)).Elem(), SparkJobDependentPackageArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkJobDependentPackageArrayInput)(nil)).Elem(), SparkJobDependentPackageArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkJobDependentPackagePackageInput)(nil)).Elem(), SparkJobDependentPackagePackageArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkJobDependentPackagePackageArrayInput)(nil)).Elem(), SparkJobDependentPackagePackageArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*SqlJobConfInput)(nil)).Elem(), SqlJobConfArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SqlJobConfPtrInput)(nil)).Elem(), SqlJobConfArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*TableColumnInput)(nil)).Elem(), TableColumnArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*TableColumnArrayInput)(nil)).Elem(), TableColumnArray{})
	pulumi.RegisterOutputType(SparkJobDependentPackageOutput{})
	pulumi.RegisterOutputType(SparkJobDependentPackageArrayOutput{})
	pulumi.RegisterOutputType(SparkJobDependentPackagePackageOutput{})
	pulumi.RegisterOutputType(SparkJobDependentPackagePackageArrayOutput{})
	pulumi.RegisterOutputType(SqlJobConfOutput{})
	pulumi.RegisterOutputType(SqlJobConfPtrOutput{})
	pulumi.RegisterOutputType(TableColumnOutput{})
	pulumi.RegisterOutputType(TableColumnArrayOutput{})
}
