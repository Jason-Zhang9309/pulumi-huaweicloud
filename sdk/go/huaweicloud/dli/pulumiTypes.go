// Code generated by the Pulumi Terraform Bridge (tfgen) Tool DO NOT EDIT.
// *** WARNING: Do not edit by hand unless you're certain you know what you are doing! ***

package dli

import (
	"context"
	"reflect"

	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
)

type QueueScalingPolicy struct {
	// Specifies the effective time of the queue scaling policy.
	// The value can be set only by hour.
	ImpactStartTime string `pulumi:"impactStartTime"`
	// Specifies the expiration time of the queue scaling policy.
	// The value can be set only by hour.
	ImpactStopTime string `pulumi:"impactStopTime"`
	// Specifies the maximum number of CUs allowed by the scaling policy.
	// The number must be a multiple of `4`.
	MaxCu int `pulumi:"maxCu"`
	// Specifies the minimum number of CUs allowed by the scaling policy.
	// The number must be a multiple of `4`.
	MinCu int `pulumi:"minCu"`
	// Specifies the priority of the queue scaling policy.
	// The valid value ranges from `1` to `100`. The larger value means the higher priority.
	Priority int `pulumi:"priority"`
}

// QueueScalingPolicyInput is an input type that accepts QueueScalingPolicyArgs and QueueScalingPolicyOutput values.
// You can construct a concrete instance of `QueueScalingPolicyInput` via:
//
//	QueueScalingPolicyArgs{...}
type QueueScalingPolicyInput interface {
	pulumi.Input

	ToQueueScalingPolicyOutput() QueueScalingPolicyOutput
	ToQueueScalingPolicyOutputWithContext(context.Context) QueueScalingPolicyOutput
}

type QueueScalingPolicyArgs struct {
	// Specifies the effective time of the queue scaling policy.
	// The value can be set only by hour.
	ImpactStartTime pulumi.StringInput `pulumi:"impactStartTime"`
	// Specifies the expiration time of the queue scaling policy.
	// The value can be set only by hour.
	ImpactStopTime pulumi.StringInput `pulumi:"impactStopTime"`
	// Specifies the maximum number of CUs allowed by the scaling policy.
	// The number must be a multiple of `4`.
	MaxCu pulumi.IntInput `pulumi:"maxCu"`
	// Specifies the minimum number of CUs allowed by the scaling policy.
	// The number must be a multiple of `4`.
	MinCu pulumi.IntInput `pulumi:"minCu"`
	// Specifies the priority of the queue scaling policy.
	// The valid value ranges from `1` to `100`. The larger value means the higher priority.
	Priority pulumi.IntInput `pulumi:"priority"`
}

func (QueueScalingPolicyArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*QueueScalingPolicy)(nil)).Elem()
}

func (i QueueScalingPolicyArgs) ToQueueScalingPolicyOutput() QueueScalingPolicyOutput {
	return i.ToQueueScalingPolicyOutputWithContext(context.Background())
}

func (i QueueScalingPolicyArgs) ToQueueScalingPolicyOutputWithContext(ctx context.Context) QueueScalingPolicyOutput {
	return pulumi.ToOutputWithContext(ctx, i).(QueueScalingPolicyOutput)
}

// QueueScalingPolicyArrayInput is an input type that accepts QueueScalingPolicyArray and QueueScalingPolicyArrayOutput values.
// You can construct a concrete instance of `QueueScalingPolicyArrayInput` via:
//
//	QueueScalingPolicyArray{ QueueScalingPolicyArgs{...} }
type QueueScalingPolicyArrayInput interface {
	pulumi.Input

	ToQueueScalingPolicyArrayOutput() QueueScalingPolicyArrayOutput
	ToQueueScalingPolicyArrayOutputWithContext(context.Context) QueueScalingPolicyArrayOutput
}

type QueueScalingPolicyArray []QueueScalingPolicyInput

func (QueueScalingPolicyArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]QueueScalingPolicy)(nil)).Elem()
}

func (i QueueScalingPolicyArray) ToQueueScalingPolicyArrayOutput() QueueScalingPolicyArrayOutput {
	return i.ToQueueScalingPolicyArrayOutputWithContext(context.Background())
}

func (i QueueScalingPolicyArray) ToQueueScalingPolicyArrayOutputWithContext(ctx context.Context) QueueScalingPolicyArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(QueueScalingPolicyArrayOutput)
}

type QueueScalingPolicyOutput struct{ *pulumi.OutputState }

func (QueueScalingPolicyOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*QueueScalingPolicy)(nil)).Elem()
}

func (o QueueScalingPolicyOutput) ToQueueScalingPolicyOutput() QueueScalingPolicyOutput {
	return o
}

func (o QueueScalingPolicyOutput) ToQueueScalingPolicyOutputWithContext(ctx context.Context) QueueScalingPolicyOutput {
	return o
}

// Specifies the effective time of the queue scaling policy.
// The value can be set only by hour.
func (o QueueScalingPolicyOutput) ImpactStartTime() pulumi.StringOutput {
	return o.ApplyT(func(v QueueScalingPolicy) string { return v.ImpactStartTime }).(pulumi.StringOutput)
}

// Specifies the expiration time of the queue scaling policy.
// The value can be set only by hour.
func (o QueueScalingPolicyOutput) ImpactStopTime() pulumi.StringOutput {
	return o.ApplyT(func(v QueueScalingPolicy) string { return v.ImpactStopTime }).(pulumi.StringOutput)
}

// Specifies the maximum number of CUs allowed by the scaling policy.
// The number must be a multiple of `4`.
func (o QueueScalingPolicyOutput) MaxCu() pulumi.IntOutput {
	return o.ApplyT(func(v QueueScalingPolicy) int { return v.MaxCu }).(pulumi.IntOutput)
}

// Specifies the minimum number of CUs allowed by the scaling policy.
// The number must be a multiple of `4`.
func (o QueueScalingPolicyOutput) MinCu() pulumi.IntOutput {
	return o.ApplyT(func(v QueueScalingPolicy) int { return v.MinCu }).(pulumi.IntOutput)
}

// Specifies the priority of the queue scaling policy.
// The valid value ranges from `1` to `100`. The larger value means the higher priority.
func (o QueueScalingPolicyOutput) Priority() pulumi.IntOutput {
	return o.ApplyT(func(v QueueScalingPolicy) int { return v.Priority }).(pulumi.IntOutput)
}

type QueueScalingPolicyArrayOutput struct{ *pulumi.OutputState }

func (QueueScalingPolicyArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]QueueScalingPolicy)(nil)).Elem()
}

func (o QueueScalingPolicyArrayOutput) ToQueueScalingPolicyArrayOutput() QueueScalingPolicyArrayOutput {
	return o
}

func (o QueueScalingPolicyArrayOutput) ToQueueScalingPolicyArrayOutputWithContext(ctx context.Context) QueueScalingPolicyArrayOutput {
	return o
}

func (o QueueScalingPolicyArrayOutput) Index(i pulumi.IntInput) QueueScalingPolicyOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) QueueScalingPolicy {
		return vs[0].([]QueueScalingPolicy)[vs[1].(int)]
	}).(QueueScalingPolicyOutput)
}

type QueueSparkDriver struct {
	// Specifies the maximum number of tasks that can be concurrently executed by a spark driver.
	// The valid value ranges from `1` to `32`.
	MaxConcurrent *int `pulumi:"maxConcurrent"`
	// Specifies the maximum number of spark drivers that can be started on the queue.
	// If the `cuCount` is `16`, the value can only be `2`.
	// If The `cuCount` is greater than `16`, the minimum value is `2`, the maximum value is the number of queue CUs
	// divided by `16`.
	MaxInstance *int `pulumi:"maxInstance"`
	// Specifies the maximum number of spark drivers to be pre-started on the queue.
	// The minimum value is `0`. If the `cuCount` is less than `32`, the maximum value is `1`.
	// If the `cuCount` is greater than or equal to `32`, the maximum value is the number of queue CUs divided by `16`.
	MaxPrefetchInstance *string `pulumi:"maxPrefetchInstance"`
}

// QueueSparkDriverInput is an input type that accepts QueueSparkDriverArgs and QueueSparkDriverOutput values.
// You can construct a concrete instance of `QueueSparkDriverInput` via:
//
//	QueueSparkDriverArgs{...}
type QueueSparkDriverInput interface {
	pulumi.Input

	ToQueueSparkDriverOutput() QueueSparkDriverOutput
	ToQueueSparkDriverOutputWithContext(context.Context) QueueSparkDriverOutput
}

type QueueSparkDriverArgs struct {
	// Specifies the maximum number of tasks that can be concurrently executed by a spark driver.
	// The valid value ranges from `1` to `32`.
	MaxConcurrent pulumi.IntPtrInput `pulumi:"maxConcurrent"`
	// Specifies the maximum number of spark drivers that can be started on the queue.
	// If the `cuCount` is `16`, the value can only be `2`.
	// If The `cuCount` is greater than `16`, the minimum value is `2`, the maximum value is the number of queue CUs
	// divided by `16`.
	MaxInstance pulumi.IntPtrInput `pulumi:"maxInstance"`
	// Specifies the maximum number of spark drivers to be pre-started on the queue.
	// The minimum value is `0`. If the `cuCount` is less than `32`, the maximum value is `1`.
	// If the `cuCount` is greater than or equal to `32`, the maximum value is the number of queue CUs divided by `16`.
	MaxPrefetchInstance pulumi.StringPtrInput `pulumi:"maxPrefetchInstance"`
}

func (QueueSparkDriverArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*QueueSparkDriver)(nil)).Elem()
}

func (i QueueSparkDriverArgs) ToQueueSparkDriverOutput() QueueSparkDriverOutput {
	return i.ToQueueSparkDriverOutputWithContext(context.Background())
}

func (i QueueSparkDriverArgs) ToQueueSparkDriverOutputWithContext(ctx context.Context) QueueSparkDriverOutput {
	return pulumi.ToOutputWithContext(ctx, i).(QueueSparkDriverOutput)
}

func (i QueueSparkDriverArgs) ToQueueSparkDriverPtrOutput() QueueSparkDriverPtrOutput {
	return i.ToQueueSparkDriverPtrOutputWithContext(context.Background())
}

func (i QueueSparkDriverArgs) ToQueueSparkDriverPtrOutputWithContext(ctx context.Context) QueueSparkDriverPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(QueueSparkDriverOutput).ToQueueSparkDriverPtrOutputWithContext(ctx)
}

// QueueSparkDriverPtrInput is an input type that accepts QueueSparkDriverArgs, QueueSparkDriverPtr and QueueSparkDriverPtrOutput values.
// You can construct a concrete instance of `QueueSparkDriverPtrInput` via:
//
//	        QueueSparkDriverArgs{...}
//
//	or:
//
//	        nil
type QueueSparkDriverPtrInput interface {
	pulumi.Input

	ToQueueSparkDriverPtrOutput() QueueSparkDriverPtrOutput
	ToQueueSparkDriverPtrOutputWithContext(context.Context) QueueSparkDriverPtrOutput
}

type queueSparkDriverPtrType QueueSparkDriverArgs

func QueueSparkDriverPtr(v *QueueSparkDriverArgs) QueueSparkDriverPtrInput {
	return (*queueSparkDriverPtrType)(v)
}

func (*queueSparkDriverPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**QueueSparkDriver)(nil)).Elem()
}

func (i *queueSparkDriverPtrType) ToQueueSparkDriverPtrOutput() QueueSparkDriverPtrOutput {
	return i.ToQueueSparkDriverPtrOutputWithContext(context.Background())
}

func (i *queueSparkDriverPtrType) ToQueueSparkDriverPtrOutputWithContext(ctx context.Context) QueueSparkDriverPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(QueueSparkDriverPtrOutput)
}

type QueueSparkDriverOutput struct{ *pulumi.OutputState }

func (QueueSparkDriverOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*QueueSparkDriver)(nil)).Elem()
}

func (o QueueSparkDriverOutput) ToQueueSparkDriverOutput() QueueSparkDriverOutput {
	return o
}

func (o QueueSparkDriverOutput) ToQueueSparkDriverOutputWithContext(ctx context.Context) QueueSparkDriverOutput {
	return o
}

func (o QueueSparkDriverOutput) ToQueueSparkDriverPtrOutput() QueueSparkDriverPtrOutput {
	return o.ToQueueSparkDriverPtrOutputWithContext(context.Background())
}

func (o QueueSparkDriverOutput) ToQueueSparkDriverPtrOutputWithContext(ctx context.Context) QueueSparkDriverPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v QueueSparkDriver) *QueueSparkDriver {
		return &v
	}).(QueueSparkDriverPtrOutput)
}

// Specifies the maximum number of tasks that can be concurrently executed by a spark driver.
// The valid value ranges from `1` to `32`.
func (o QueueSparkDriverOutput) MaxConcurrent() pulumi.IntPtrOutput {
	return o.ApplyT(func(v QueueSparkDriver) *int { return v.MaxConcurrent }).(pulumi.IntPtrOutput)
}

// Specifies the maximum number of spark drivers that can be started on the queue.
// If the `cuCount` is `16`, the value can only be `2`.
// If The `cuCount` is greater than `16`, the minimum value is `2`, the maximum value is the number of queue CUs
// divided by `16`.
func (o QueueSparkDriverOutput) MaxInstance() pulumi.IntPtrOutput {
	return o.ApplyT(func(v QueueSparkDriver) *int { return v.MaxInstance }).(pulumi.IntPtrOutput)
}

// Specifies the maximum number of spark drivers to be pre-started on the queue.
// The minimum value is `0`. If the `cuCount` is less than `32`, the maximum value is `1`.
// If the `cuCount` is greater than or equal to `32`, the maximum value is the number of queue CUs divided by `16`.
func (o QueueSparkDriverOutput) MaxPrefetchInstance() pulumi.StringPtrOutput {
	return o.ApplyT(func(v QueueSparkDriver) *string { return v.MaxPrefetchInstance }).(pulumi.StringPtrOutput)
}

type QueueSparkDriverPtrOutput struct{ *pulumi.OutputState }

func (QueueSparkDriverPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**QueueSparkDriver)(nil)).Elem()
}

func (o QueueSparkDriverPtrOutput) ToQueueSparkDriverPtrOutput() QueueSparkDriverPtrOutput {
	return o
}

func (o QueueSparkDriverPtrOutput) ToQueueSparkDriverPtrOutputWithContext(ctx context.Context) QueueSparkDriverPtrOutput {
	return o
}

func (o QueueSparkDriverPtrOutput) Elem() QueueSparkDriverOutput {
	return o.ApplyT(func(v *QueueSparkDriver) QueueSparkDriver {
		if v != nil {
			return *v
		}
		var ret QueueSparkDriver
		return ret
	}).(QueueSparkDriverOutput)
}

// Specifies the maximum number of tasks that can be concurrently executed by a spark driver.
// The valid value ranges from `1` to `32`.
func (o QueueSparkDriverPtrOutput) MaxConcurrent() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *QueueSparkDriver) *int {
		if v == nil {
			return nil
		}
		return v.MaxConcurrent
	}).(pulumi.IntPtrOutput)
}

// Specifies the maximum number of spark drivers that can be started on the queue.
// If the `cuCount` is `16`, the value can only be `2`.
// If The `cuCount` is greater than `16`, the minimum value is `2`, the maximum value is the number of queue CUs
// divided by `16`.
func (o QueueSparkDriverPtrOutput) MaxInstance() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *QueueSparkDriver) *int {
		if v == nil {
			return nil
		}
		return v.MaxInstance
	}).(pulumi.IntPtrOutput)
}

// Specifies the maximum number of spark drivers to be pre-started on the queue.
// The minimum value is `0`. If the `cuCount` is less than `32`, the maximum value is `1`.
// If the `cuCount` is greater than or equal to `32`, the maximum value is the number of queue CUs divided by `16`.
func (o QueueSparkDriverPtrOutput) MaxPrefetchInstance() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *QueueSparkDriver) *string {
		if v == nil {
			return nil
		}
		return v.MaxPrefetchInstance
	}).(pulumi.StringPtrOutput)
}

type SparkJobDependentPackage struct {
	// Specifies the user group name.\
	// Only letters, digits, dots (.), hyphens (-) and underscores (_) are allowed.
	// Changing this parameter will submit a new spark job.
	GroupName string `pulumi:"groupName"`
	// Specifies the user group resource for details.
	// Changing this parameter will submit a new spark job.
	// The object structure is documented below.
	Packages []SparkJobDependentPackagePackage `pulumi:"packages"`
}

// SparkJobDependentPackageInput is an input type that accepts SparkJobDependentPackageArgs and SparkJobDependentPackageOutput values.
// You can construct a concrete instance of `SparkJobDependentPackageInput` via:
//
//	SparkJobDependentPackageArgs{...}
type SparkJobDependentPackageInput interface {
	pulumi.Input

	ToSparkJobDependentPackageOutput() SparkJobDependentPackageOutput
	ToSparkJobDependentPackageOutputWithContext(context.Context) SparkJobDependentPackageOutput
}

type SparkJobDependentPackageArgs struct {
	// Specifies the user group name.\
	// Only letters, digits, dots (.), hyphens (-) and underscores (_) are allowed.
	// Changing this parameter will submit a new spark job.
	GroupName pulumi.StringInput `pulumi:"groupName"`
	// Specifies the user group resource for details.
	// Changing this parameter will submit a new spark job.
	// The object structure is documented below.
	Packages SparkJobDependentPackagePackageArrayInput `pulumi:"packages"`
}

func (SparkJobDependentPackageArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkJobDependentPackage)(nil)).Elem()
}

func (i SparkJobDependentPackageArgs) ToSparkJobDependentPackageOutput() SparkJobDependentPackageOutput {
	return i.ToSparkJobDependentPackageOutputWithContext(context.Background())
}

func (i SparkJobDependentPackageArgs) ToSparkJobDependentPackageOutputWithContext(ctx context.Context) SparkJobDependentPackageOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkJobDependentPackageOutput)
}

// SparkJobDependentPackageArrayInput is an input type that accepts SparkJobDependentPackageArray and SparkJobDependentPackageArrayOutput values.
// You can construct a concrete instance of `SparkJobDependentPackageArrayInput` via:
//
//	SparkJobDependentPackageArray{ SparkJobDependentPackageArgs{...} }
type SparkJobDependentPackageArrayInput interface {
	pulumi.Input

	ToSparkJobDependentPackageArrayOutput() SparkJobDependentPackageArrayOutput
	ToSparkJobDependentPackageArrayOutputWithContext(context.Context) SparkJobDependentPackageArrayOutput
}

type SparkJobDependentPackageArray []SparkJobDependentPackageInput

func (SparkJobDependentPackageArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]SparkJobDependentPackage)(nil)).Elem()
}

func (i SparkJobDependentPackageArray) ToSparkJobDependentPackageArrayOutput() SparkJobDependentPackageArrayOutput {
	return i.ToSparkJobDependentPackageArrayOutputWithContext(context.Background())
}

func (i SparkJobDependentPackageArray) ToSparkJobDependentPackageArrayOutputWithContext(ctx context.Context) SparkJobDependentPackageArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkJobDependentPackageArrayOutput)
}

type SparkJobDependentPackageOutput struct{ *pulumi.OutputState }

func (SparkJobDependentPackageOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkJobDependentPackage)(nil)).Elem()
}

func (o SparkJobDependentPackageOutput) ToSparkJobDependentPackageOutput() SparkJobDependentPackageOutput {
	return o
}

func (o SparkJobDependentPackageOutput) ToSparkJobDependentPackageOutputWithContext(ctx context.Context) SparkJobDependentPackageOutput {
	return o
}

// Specifies the user group name.\
// Only letters, digits, dots (.), hyphens (-) and underscores (_) are allowed.
// Changing this parameter will submit a new spark job.
func (o SparkJobDependentPackageOutput) GroupName() pulumi.StringOutput {
	return o.ApplyT(func(v SparkJobDependentPackage) string { return v.GroupName }).(pulumi.StringOutput)
}

// Specifies the user group resource for details.
// Changing this parameter will submit a new spark job.
// The object structure is documented below.
func (o SparkJobDependentPackageOutput) Packages() SparkJobDependentPackagePackageArrayOutput {
	return o.ApplyT(func(v SparkJobDependentPackage) []SparkJobDependentPackagePackage { return v.Packages }).(SparkJobDependentPackagePackageArrayOutput)
}

type SparkJobDependentPackageArrayOutput struct{ *pulumi.OutputState }

func (SparkJobDependentPackageArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]SparkJobDependentPackage)(nil)).Elem()
}

func (o SparkJobDependentPackageArrayOutput) ToSparkJobDependentPackageArrayOutput() SparkJobDependentPackageArrayOutput {
	return o
}

func (o SparkJobDependentPackageArrayOutput) ToSparkJobDependentPackageArrayOutputWithContext(ctx context.Context) SparkJobDependentPackageArrayOutput {
	return o
}

func (o SparkJobDependentPackageArrayOutput) Index(i pulumi.IntInput) SparkJobDependentPackageOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) SparkJobDependentPackage {
		return vs[0].([]SparkJobDependentPackage)[vs[1].(int)]
	}).(SparkJobDependentPackageOutput)
}

type SparkJobDependentPackagePackage struct {
	// Specifies the resource name of the package.
	// Changing this parameter will submit a new spark job.
	PackageName string `pulumi:"packageName"`
	// Specifies the resource type of the package.
	// Changing this parameter will submit a new spark job.
	Type string `pulumi:"type"`
}

// SparkJobDependentPackagePackageInput is an input type that accepts SparkJobDependentPackagePackageArgs and SparkJobDependentPackagePackageOutput values.
// You can construct a concrete instance of `SparkJobDependentPackagePackageInput` via:
//
//	SparkJobDependentPackagePackageArgs{...}
type SparkJobDependentPackagePackageInput interface {
	pulumi.Input

	ToSparkJobDependentPackagePackageOutput() SparkJobDependentPackagePackageOutput
	ToSparkJobDependentPackagePackageOutputWithContext(context.Context) SparkJobDependentPackagePackageOutput
}

type SparkJobDependentPackagePackageArgs struct {
	// Specifies the resource name of the package.
	// Changing this parameter will submit a new spark job.
	PackageName pulumi.StringInput `pulumi:"packageName"`
	// Specifies the resource type of the package.
	// Changing this parameter will submit a new spark job.
	Type pulumi.StringInput `pulumi:"type"`
}

func (SparkJobDependentPackagePackageArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkJobDependentPackagePackage)(nil)).Elem()
}

func (i SparkJobDependentPackagePackageArgs) ToSparkJobDependentPackagePackageOutput() SparkJobDependentPackagePackageOutput {
	return i.ToSparkJobDependentPackagePackageOutputWithContext(context.Background())
}

func (i SparkJobDependentPackagePackageArgs) ToSparkJobDependentPackagePackageOutputWithContext(ctx context.Context) SparkJobDependentPackagePackageOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkJobDependentPackagePackageOutput)
}

// SparkJobDependentPackagePackageArrayInput is an input type that accepts SparkJobDependentPackagePackageArray and SparkJobDependentPackagePackageArrayOutput values.
// You can construct a concrete instance of `SparkJobDependentPackagePackageArrayInput` via:
//
//	SparkJobDependentPackagePackageArray{ SparkJobDependentPackagePackageArgs{...} }
type SparkJobDependentPackagePackageArrayInput interface {
	pulumi.Input

	ToSparkJobDependentPackagePackageArrayOutput() SparkJobDependentPackagePackageArrayOutput
	ToSparkJobDependentPackagePackageArrayOutputWithContext(context.Context) SparkJobDependentPackagePackageArrayOutput
}

type SparkJobDependentPackagePackageArray []SparkJobDependentPackagePackageInput

func (SparkJobDependentPackagePackageArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]SparkJobDependentPackagePackage)(nil)).Elem()
}

func (i SparkJobDependentPackagePackageArray) ToSparkJobDependentPackagePackageArrayOutput() SparkJobDependentPackagePackageArrayOutput {
	return i.ToSparkJobDependentPackagePackageArrayOutputWithContext(context.Background())
}

func (i SparkJobDependentPackagePackageArray) ToSparkJobDependentPackagePackageArrayOutputWithContext(ctx context.Context) SparkJobDependentPackagePackageArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkJobDependentPackagePackageArrayOutput)
}

type SparkJobDependentPackagePackageOutput struct{ *pulumi.OutputState }

func (SparkJobDependentPackagePackageOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkJobDependentPackagePackage)(nil)).Elem()
}

func (o SparkJobDependentPackagePackageOutput) ToSparkJobDependentPackagePackageOutput() SparkJobDependentPackagePackageOutput {
	return o
}

func (o SparkJobDependentPackagePackageOutput) ToSparkJobDependentPackagePackageOutputWithContext(ctx context.Context) SparkJobDependentPackagePackageOutput {
	return o
}

// Specifies the resource name of the package.
// Changing this parameter will submit a new spark job.
func (o SparkJobDependentPackagePackageOutput) PackageName() pulumi.StringOutput {
	return o.ApplyT(func(v SparkJobDependentPackagePackage) string { return v.PackageName }).(pulumi.StringOutput)
}

// Specifies the resource type of the package.
// Changing this parameter will submit a new spark job.
func (o SparkJobDependentPackagePackageOutput) Type() pulumi.StringOutput {
	return o.ApplyT(func(v SparkJobDependentPackagePackage) string { return v.Type }).(pulumi.StringOutput)
}

type SparkJobDependentPackagePackageArrayOutput struct{ *pulumi.OutputState }

func (SparkJobDependentPackagePackageArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]SparkJobDependentPackagePackage)(nil)).Elem()
}

func (o SparkJobDependentPackagePackageArrayOutput) ToSparkJobDependentPackagePackageArrayOutput() SparkJobDependentPackagePackageArrayOutput {
	return o
}

func (o SparkJobDependentPackagePackageArrayOutput) ToSparkJobDependentPackagePackageArrayOutputWithContext(ctx context.Context) SparkJobDependentPackagePackageArrayOutput {
	return o
}

func (o SparkJobDependentPackagePackageArrayOutput) Index(i pulumi.IntInput) SparkJobDependentPackagePackageOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) SparkJobDependentPackagePackage {
		return vs[0].([]SparkJobDependentPackagePackage)[vs[1].(int)]
	}).(SparkJobDependentPackagePackageOutput)
}

type SqlJobConf struct {
	// Sets the job running timeout interval. If the timeout interval
	// expires, the job is canceled. Unit: `ms`. Changing this parameter will create a new resource.
	DliSqlJobTimeout *int `pulumi:"dliSqlJobTimeout"`
	// Specifies whether DDL and DCL statements are executed
	// asynchronously. The value true indicates that asynchronous execution is enabled. Default value is `false`.
	// Changing this parameter will create a new resource.
	DliSqlSqlasyncEnabled *bool `pulumi:"dliSqlSqlasyncEnabled"`
	// Maximum size of the table that
	// displays all working nodes when a connection is executed. You can set this parameter to -1 to disable the display.
	// Default value is `209715200`. Changing this parameter will create a new resource.
	SparkSqlAutoBroadcastJoinThreshold *int `pulumi:"sparkSqlAutoBroadcastJoinThreshold"`
	// Path of bad records. Changing this parameter will create
	// a new resource.
	SparkSqlBadRecordsPath *string `pulumi:"sparkSqlBadRecordsPath"`
	// In dynamic mode, Spark does not delete
	// the previous partitions and only overwrites the partitions without data during execution. Default value is `false`.
	// Changing this parameter will create a new resource.
	SparkSqlDynamicPartitionOverwriteEnabled *bool `pulumi:"sparkSqlDynamicPartitionOverwriteEnabled"`
	// Maximum number of bytes to be packed into a
	// single partition when a file is read. Default value is `134217728`. Changing this parameter will create a new
	// resource.
	SparkSqlFilesMaxPartitionBytes *int `pulumi:"sparkSqlFilesMaxPartitionBytes"`
	// Maximum number of records to be written
	// into a single file. If the value is zero or negative, there is no limit. Default value is `0`.
	// Changing this parameter will create a new resource.
	SparkSqlMaxRecordsPerFile *int `pulumi:"sparkSqlMaxRecordsPerFile"`
	// Default number of partitions used to filter
	// data for join or aggregation. Default value is `4096`. Changing this parameter will create a new resource.
	SparkSqlShufflePartitions *int `pulumi:"sparkSqlShufflePartitions"`
}

// SqlJobConfInput is an input type that accepts SqlJobConfArgs and SqlJobConfOutput values.
// You can construct a concrete instance of `SqlJobConfInput` via:
//
//	SqlJobConfArgs{...}
type SqlJobConfInput interface {
	pulumi.Input

	ToSqlJobConfOutput() SqlJobConfOutput
	ToSqlJobConfOutputWithContext(context.Context) SqlJobConfOutput
}

type SqlJobConfArgs struct {
	// Sets the job running timeout interval. If the timeout interval
	// expires, the job is canceled. Unit: `ms`. Changing this parameter will create a new resource.
	DliSqlJobTimeout pulumi.IntPtrInput `pulumi:"dliSqlJobTimeout"`
	// Specifies whether DDL and DCL statements are executed
	// asynchronously. The value true indicates that asynchronous execution is enabled. Default value is `false`.
	// Changing this parameter will create a new resource.
	DliSqlSqlasyncEnabled pulumi.BoolPtrInput `pulumi:"dliSqlSqlasyncEnabled"`
	// Maximum size of the table that
	// displays all working nodes when a connection is executed. You can set this parameter to -1 to disable the display.
	// Default value is `209715200`. Changing this parameter will create a new resource.
	SparkSqlAutoBroadcastJoinThreshold pulumi.IntPtrInput `pulumi:"sparkSqlAutoBroadcastJoinThreshold"`
	// Path of bad records. Changing this parameter will create
	// a new resource.
	SparkSqlBadRecordsPath pulumi.StringPtrInput `pulumi:"sparkSqlBadRecordsPath"`
	// In dynamic mode, Spark does not delete
	// the previous partitions and only overwrites the partitions without data during execution. Default value is `false`.
	// Changing this parameter will create a new resource.
	SparkSqlDynamicPartitionOverwriteEnabled pulumi.BoolPtrInput `pulumi:"sparkSqlDynamicPartitionOverwriteEnabled"`
	// Maximum number of bytes to be packed into a
	// single partition when a file is read. Default value is `134217728`. Changing this parameter will create a new
	// resource.
	SparkSqlFilesMaxPartitionBytes pulumi.IntPtrInput `pulumi:"sparkSqlFilesMaxPartitionBytes"`
	// Maximum number of records to be written
	// into a single file. If the value is zero or negative, there is no limit. Default value is `0`.
	// Changing this parameter will create a new resource.
	SparkSqlMaxRecordsPerFile pulumi.IntPtrInput `pulumi:"sparkSqlMaxRecordsPerFile"`
	// Default number of partitions used to filter
	// data for join or aggregation. Default value is `4096`. Changing this parameter will create a new resource.
	SparkSqlShufflePartitions pulumi.IntPtrInput `pulumi:"sparkSqlShufflePartitions"`
}

func (SqlJobConfArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*SqlJobConf)(nil)).Elem()
}

func (i SqlJobConfArgs) ToSqlJobConfOutput() SqlJobConfOutput {
	return i.ToSqlJobConfOutputWithContext(context.Background())
}

func (i SqlJobConfArgs) ToSqlJobConfOutputWithContext(ctx context.Context) SqlJobConfOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SqlJobConfOutput)
}

func (i SqlJobConfArgs) ToSqlJobConfPtrOutput() SqlJobConfPtrOutput {
	return i.ToSqlJobConfPtrOutputWithContext(context.Background())
}

func (i SqlJobConfArgs) ToSqlJobConfPtrOutputWithContext(ctx context.Context) SqlJobConfPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SqlJobConfOutput).ToSqlJobConfPtrOutputWithContext(ctx)
}

// SqlJobConfPtrInput is an input type that accepts SqlJobConfArgs, SqlJobConfPtr and SqlJobConfPtrOutput values.
// You can construct a concrete instance of `SqlJobConfPtrInput` via:
//
//	        SqlJobConfArgs{...}
//
//	or:
//
//	        nil
type SqlJobConfPtrInput interface {
	pulumi.Input

	ToSqlJobConfPtrOutput() SqlJobConfPtrOutput
	ToSqlJobConfPtrOutputWithContext(context.Context) SqlJobConfPtrOutput
}

type sqlJobConfPtrType SqlJobConfArgs

func SqlJobConfPtr(v *SqlJobConfArgs) SqlJobConfPtrInput {
	return (*sqlJobConfPtrType)(v)
}

func (*sqlJobConfPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**SqlJobConf)(nil)).Elem()
}

func (i *sqlJobConfPtrType) ToSqlJobConfPtrOutput() SqlJobConfPtrOutput {
	return i.ToSqlJobConfPtrOutputWithContext(context.Background())
}

func (i *sqlJobConfPtrType) ToSqlJobConfPtrOutputWithContext(ctx context.Context) SqlJobConfPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SqlJobConfPtrOutput)
}

type SqlJobConfOutput struct{ *pulumi.OutputState }

func (SqlJobConfOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SqlJobConf)(nil)).Elem()
}

func (o SqlJobConfOutput) ToSqlJobConfOutput() SqlJobConfOutput {
	return o
}

func (o SqlJobConfOutput) ToSqlJobConfOutputWithContext(ctx context.Context) SqlJobConfOutput {
	return o
}

func (o SqlJobConfOutput) ToSqlJobConfPtrOutput() SqlJobConfPtrOutput {
	return o.ToSqlJobConfPtrOutputWithContext(context.Background())
}

func (o SqlJobConfOutput) ToSqlJobConfPtrOutputWithContext(ctx context.Context) SqlJobConfPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v SqlJobConf) *SqlJobConf {
		return &v
	}).(SqlJobConfPtrOutput)
}

// Sets the job running timeout interval. If the timeout interval
// expires, the job is canceled. Unit: `ms`. Changing this parameter will create a new resource.
func (o SqlJobConfOutput) DliSqlJobTimeout() pulumi.IntPtrOutput {
	return o.ApplyT(func(v SqlJobConf) *int { return v.DliSqlJobTimeout }).(pulumi.IntPtrOutput)
}

// Specifies whether DDL and DCL statements are executed
// asynchronously. The value true indicates that asynchronous execution is enabled. Default value is `false`.
// Changing this parameter will create a new resource.
func (o SqlJobConfOutput) DliSqlSqlasyncEnabled() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v SqlJobConf) *bool { return v.DliSqlSqlasyncEnabled }).(pulumi.BoolPtrOutput)
}

// Maximum size of the table that
// displays all working nodes when a connection is executed. You can set this parameter to -1 to disable the display.
// Default value is `209715200`. Changing this parameter will create a new resource.
func (o SqlJobConfOutput) SparkSqlAutoBroadcastJoinThreshold() pulumi.IntPtrOutput {
	return o.ApplyT(func(v SqlJobConf) *int { return v.SparkSqlAutoBroadcastJoinThreshold }).(pulumi.IntPtrOutput)
}

// Path of bad records. Changing this parameter will create
// a new resource.
func (o SqlJobConfOutput) SparkSqlBadRecordsPath() pulumi.StringPtrOutput {
	return o.ApplyT(func(v SqlJobConf) *string { return v.SparkSqlBadRecordsPath }).(pulumi.StringPtrOutput)
}

// In dynamic mode, Spark does not delete
// the previous partitions and only overwrites the partitions without data during execution. Default value is `false`.
// Changing this parameter will create a new resource.
func (o SqlJobConfOutput) SparkSqlDynamicPartitionOverwriteEnabled() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v SqlJobConf) *bool { return v.SparkSqlDynamicPartitionOverwriteEnabled }).(pulumi.BoolPtrOutput)
}

// Maximum number of bytes to be packed into a
// single partition when a file is read. Default value is `134217728`. Changing this parameter will create a new
// resource.
func (o SqlJobConfOutput) SparkSqlFilesMaxPartitionBytes() pulumi.IntPtrOutput {
	return o.ApplyT(func(v SqlJobConf) *int { return v.SparkSqlFilesMaxPartitionBytes }).(pulumi.IntPtrOutput)
}

// Maximum number of records to be written
// into a single file. If the value is zero or negative, there is no limit. Default value is `0`.
// Changing this parameter will create a new resource.
func (o SqlJobConfOutput) SparkSqlMaxRecordsPerFile() pulumi.IntPtrOutput {
	return o.ApplyT(func(v SqlJobConf) *int { return v.SparkSqlMaxRecordsPerFile }).(pulumi.IntPtrOutput)
}

// Default number of partitions used to filter
// data for join or aggregation. Default value is `4096`. Changing this parameter will create a new resource.
func (o SqlJobConfOutput) SparkSqlShufflePartitions() pulumi.IntPtrOutput {
	return o.ApplyT(func(v SqlJobConf) *int { return v.SparkSqlShufflePartitions }).(pulumi.IntPtrOutput)
}

type SqlJobConfPtrOutput struct{ *pulumi.OutputState }

func (SqlJobConfPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**SqlJobConf)(nil)).Elem()
}

func (o SqlJobConfPtrOutput) ToSqlJobConfPtrOutput() SqlJobConfPtrOutput {
	return o
}

func (o SqlJobConfPtrOutput) ToSqlJobConfPtrOutputWithContext(ctx context.Context) SqlJobConfPtrOutput {
	return o
}

func (o SqlJobConfPtrOutput) Elem() SqlJobConfOutput {
	return o.ApplyT(func(v *SqlJobConf) SqlJobConf {
		if v != nil {
			return *v
		}
		var ret SqlJobConf
		return ret
	}).(SqlJobConfOutput)
}

// Sets the job running timeout interval. If the timeout interval
// expires, the job is canceled. Unit: `ms`. Changing this parameter will create a new resource.
func (o SqlJobConfPtrOutput) DliSqlJobTimeout() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *SqlJobConf) *int {
		if v == nil {
			return nil
		}
		return v.DliSqlJobTimeout
	}).(pulumi.IntPtrOutput)
}

// Specifies whether DDL and DCL statements are executed
// asynchronously. The value true indicates that asynchronous execution is enabled. Default value is `false`.
// Changing this parameter will create a new resource.
func (o SqlJobConfPtrOutput) DliSqlSqlasyncEnabled() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *SqlJobConf) *bool {
		if v == nil {
			return nil
		}
		return v.DliSqlSqlasyncEnabled
	}).(pulumi.BoolPtrOutput)
}

// Maximum size of the table that
// displays all working nodes when a connection is executed. You can set this parameter to -1 to disable the display.
// Default value is `209715200`. Changing this parameter will create a new resource.
func (o SqlJobConfPtrOutput) SparkSqlAutoBroadcastJoinThreshold() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *SqlJobConf) *int {
		if v == nil {
			return nil
		}
		return v.SparkSqlAutoBroadcastJoinThreshold
	}).(pulumi.IntPtrOutput)
}

// Path of bad records. Changing this parameter will create
// a new resource.
func (o SqlJobConfPtrOutput) SparkSqlBadRecordsPath() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *SqlJobConf) *string {
		if v == nil {
			return nil
		}
		return v.SparkSqlBadRecordsPath
	}).(pulumi.StringPtrOutput)
}

// In dynamic mode, Spark does not delete
// the previous partitions and only overwrites the partitions without data during execution. Default value is `false`.
// Changing this parameter will create a new resource.
func (o SqlJobConfPtrOutput) SparkSqlDynamicPartitionOverwriteEnabled() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *SqlJobConf) *bool {
		if v == nil {
			return nil
		}
		return v.SparkSqlDynamicPartitionOverwriteEnabled
	}).(pulumi.BoolPtrOutput)
}

// Maximum number of bytes to be packed into a
// single partition when a file is read. Default value is `134217728`. Changing this parameter will create a new
// resource.
func (o SqlJobConfPtrOutput) SparkSqlFilesMaxPartitionBytes() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *SqlJobConf) *int {
		if v == nil {
			return nil
		}
		return v.SparkSqlFilesMaxPartitionBytes
	}).(pulumi.IntPtrOutput)
}

// Maximum number of records to be written
// into a single file. If the value is zero or negative, there is no limit. Default value is `0`.
// Changing this parameter will create a new resource.
func (o SqlJobConfPtrOutput) SparkSqlMaxRecordsPerFile() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *SqlJobConf) *int {
		if v == nil {
			return nil
		}
		return v.SparkSqlMaxRecordsPerFile
	}).(pulumi.IntPtrOutput)
}

// Default number of partitions used to filter
// data for join or aggregation. Default value is `4096`. Changing this parameter will create a new resource.
func (o SqlJobConfPtrOutput) SparkSqlShufflePartitions() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *SqlJobConf) *int {
		if v == nil {
			return nil
		}
		return v.SparkSqlShufflePartitions
	}).(pulumi.IntPtrOutput)
}

type TableColumn struct {
	// Specifies the description of column. Changing this parameter will
	// create a new resource.
	Description *string `pulumi:"description"`
	// Specifies whether the column is a partition column. The value
	// `true` indicates a partition column, and the value false indicates a non-partition column. The default value
	// is false. Changing this parameter will create a new resource.
	IsPartition *bool `pulumi:"isPartition"`
	// Specifies the name of column. Changing this parameter will create a new
	// resource.
	Name string `pulumi:"name"`
	// Specifies data type of column. Changing this parameter will create a new
	// resource.
	Type string `pulumi:"type"`
}

// TableColumnInput is an input type that accepts TableColumnArgs and TableColumnOutput values.
// You can construct a concrete instance of `TableColumnInput` via:
//
//	TableColumnArgs{...}
type TableColumnInput interface {
	pulumi.Input

	ToTableColumnOutput() TableColumnOutput
	ToTableColumnOutputWithContext(context.Context) TableColumnOutput
}

type TableColumnArgs struct {
	// Specifies the description of column. Changing this parameter will
	// create a new resource.
	Description pulumi.StringPtrInput `pulumi:"description"`
	// Specifies whether the column is a partition column. The value
	// `true` indicates a partition column, and the value false indicates a non-partition column. The default value
	// is false. Changing this parameter will create a new resource.
	IsPartition pulumi.BoolPtrInput `pulumi:"isPartition"`
	// Specifies the name of column. Changing this parameter will create a new
	// resource.
	Name pulumi.StringInput `pulumi:"name"`
	// Specifies data type of column. Changing this parameter will create a new
	// resource.
	Type pulumi.StringInput `pulumi:"type"`
}

func (TableColumnArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*TableColumn)(nil)).Elem()
}

func (i TableColumnArgs) ToTableColumnOutput() TableColumnOutput {
	return i.ToTableColumnOutputWithContext(context.Background())
}

func (i TableColumnArgs) ToTableColumnOutputWithContext(ctx context.Context) TableColumnOutput {
	return pulumi.ToOutputWithContext(ctx, i).(TableColumnOutput)
}

// TableColumnArrayInput is an input type that accepts TableColumnArray and TableColumnArrayOutput values.
// You can construct a concrete instance of `TableColumnArrayInput` via:
//
//	TableColumnArray{ TableColumnArgs{...} }
type TableColumnArrayInput interface {
	pulumi.Input

	ToTableColumnArrayOutput() TableColumnArrayOutput
	ToTableColumnArrayOutputWithContext(context.Context) TableColumnArrayOutput
}

type TableColumnArray []TableColumnInput

func (TableColumnArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]TableColumn)(nil)).Elem()
}

func (i TableColumnArray) ToTableColumnArrayOutput() TableColumnArrayOutput {
	return i.ToTableColumnArrayOutputWithContext(context.Background())
}

func (i TableColumnArray) ToTableColumnArrayOutputWithContext(ctx context.Context) TableColumnArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(TableColumnArrayOutput)
}

type TableColumnOutput struct{ *pulumi.OutputState }

func (TableColumnOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*TableColumn)(nil)).Elem()
}

func (o TableColumnOutput) ToTableColumnOutput() TableColumnOutput {
	return o
}

func (o TableColumnOutput) ToTableColumnOutputWithContext(ctx context.Context) TableColumnOutput {
	return o
}

// Specifies the description of column. Changing this parameter will
// create a new resource.
func (o TableColumnOutput) Description() pulumi.StringPtrOutput {
	return o.ApplyT(func(v TableColumn) *string { return v.Description }).(pulumi.StringPtrOutput)
}

// Specifies whether the column is a partition column. The value
// `true` indicates a partition column, and the value false indicates a non-partition column. The default value
// is false. Changing this parameter will create a new resource.
func (o TableColumnOutput) IsPartition() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v TableColumn) *bool { return v.IsPartition }).(pulumi.BoolPtrOutput)
}

// Specifies the name of column. Changing this parameter will create a new
// resource.
func (o TableColumnOutput) Name() pulumi.StringOutput {
	return o.ApplyT(func(v TableColumn) string { return v.Name }).(pulumi.StringOutput)
}

// Specifies data type of column. Changing this parameter will create a new
// resource.
func (o TableColumnOutput) Type() pulumi.StringOutput {
	return o.ApplyT(func(v TableColumn) string { return v.Type }).(pulumi.StringOutput)
}

type TableColumnArrayOutput struct{ *pulumi.OutputState }

func (TableColumnArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]TableColumn)(nil)).Elem()
}

func (o TableColumnArrayOutput) ToTableColumnArrayOutput() TableColumnArrayOutput {
	return o
}

func (o TableColumnArrayOutput) ToTableColumnArrayOutputWithContext(ctx context.Context) TableColumnArrayOutput {
	return o
}

func (o TableColumnArrayOutput) Index(i pulumi.IntInput) TableColumnOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) TableColumn {
		return vs[0].([]TableColumn)[vs[1].(int)]
	}).(TableColumnOutput)
}

func init() {
	pulumi.RegisterInputType(reflect.TypeOf((*QueueScalingPolicyInput)(nil)).Elem(), QueueScalingPolicyArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*QueueScalingPolicyArrayInput)(nil)).Elem(), QueueScalingPolicyArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*QueueSparkDriverInput)(nil)).Elem(), QueueSparkDriverArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*QueueSparkDriverPtrInput)(nil)).Elem(), QueueSparkDriverArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkJobDependentPackageInput)(nil)).Elem(), SparkJobDependentPackageArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkJobDependentPackageArrayInput)(nil)).Elem(), SparkJobDependentPackageArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkJobDependentPackagePackageInput)(nil)).Elem(), SparkJobDependentPackagePackageArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkJobDependentPackagePackageArrayInput)(nil)).Elem(), SparkJobDependentPackagePackageArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*SqlJobConfInput)(nil)).Elem(), SqlJobConfArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SqlJobConfPtrInput)(nil)).Elem(), SqlJobConfArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*TableColumnInput)(nil)).Elem(), TableColumnArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*TableColumnArrayInput)(nil)).Elem(), TableColumnArray{})
	pulumi.RegisterOutputType(QueueScalingPolicyOutput{})
	pulumi.RegisterOutputType(QueueScalingPolicyArrayOutput{})
	pulumi.RegisterOutputType(QueueSparkDriverOutput{})
	pulumi.RegisterOutputType(QueueSparkDriverPtrOutput{})
	pulumi.RegisterOutputType(SparkJobDependentPackageOutput{})
	pulumi.RegisterOutputType(SparkJobDependentPackageArrayOutput{})
	pulumi.RegisterOutputType(SparkJobDependentPackagePackageOutput{})
	pulumi.RegisterOutputType(SparkJobDependentPackagePackageArrayOutput{})
	pulumi.RegisterOutputType(SqlJobConfOutput{})
	pulumi.RegisterOutputType(SqlJobConfPtrOutput{})
	pulumi.RegisterOutputType(TableColumnOutput{})
	pulumi.RegisterOutputType(TableColumnArrayOutput{})
}
